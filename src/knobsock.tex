\documentclass{llncs}
\usepackage{amsmath,amssymb,algorithm,algorithmic}
\usepackage{hyperref,color}
% No page numbers.
\usepackage{subfig}
\usepackage{graphicx}

\newcommand{\tool}{\text{ANTsR}}
\newcommand{\R}{{\bf R}}
\newcommand{\X}{{\bf X}}
\newcommand{\Xh}{{\hat{{\bf X}}}}
\newcommand{\x}{{\bf x}}
\newcommand{\Y}{{\bf Y}}
% \newcommand{\U}{{\bf U}}
\newcommand{\V}{{\bf V}}
\newcommand{\E}{{\bf E}}
\newcommand{\y}{{\bf y}}
\newcommand{\Z}{{\bf Z}}
\newcommand{\z}{{\bf z}}
\newcommand{\bSigma}{\boldsymbol \Sigma}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\field}[1]{\mathbf{#1}}
\newcommand{\image}[1]{#1}
\newcommand{\I}{\image{I}}
\newcommand{\J}{\image{J}}
\renewcommand{\u}{\vect{u}}
\renewcommand{\v}{\vect{v}}
\renewcommand{\c}{\vect{c}}
\newcommand{\h}{\vect{h}}
\newcommand{\w}{\vect{w}}
\newcommand{\myphi}{\phi}
\newcommand{\mypsi}{\psi}
\newcommand{\D}{D}
\renewcommand{\d}{\nabla}
\newcommand{\dd}{\text{d}}
\newcommand{\p}{\partial}
\renewcommand{\L}{\Delta} % laplacian
\newcommand{\myS}{S}
\newcommand{\myR}{R}
\newcommand{\myE}{E}
\newcommand{\ld}{\langle}
\newcommand{\rd}{\rangle}
\newcommand{\LL}{\mathcal{L}} % operator L
\newcommand{\tQ}{\mathcal{Q}}
\newcommand{\Id}{\text{Id}}
\newcommand{\tG}{{G}} % operator L
\newcommand{\Diff}{\text{Diff}}
\newcommand{\VV}{\mathcal{V}}
\newcommand{\opL}{\mathcal{L}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\tk}{~ITK$^{\text{4}}$~}
\newcommand{\bsp}{$\substack{
   \rightsquigarrow \\
   b
  }$}
\newcommand{\mi}{$\substack{
   \approx \\
   \text{mi}
  }$}
\newcommand{\cc}{$\substack{
   \approx \\
   \text{cc}
  }$}
\usepackage{setspace,verbatim}
\begin{document}
\vspace{-0.1in}
\title{Standardized Registration Methods for the SATA Challenge Datasets}
\author{Brian B. Avants$^1$, Nicholas J. Tustison$^2$, Hongzhi
  Wang$^1$ \\and the
  $^5$\href{http://www.insightsoftwareconsortium.org/}{Insight Software Consortium}\\
$^1$Penn Image Computing and Science Lab \\ Dept. of Radiology \\University of
  Pennsylvania, Philadelphia, PA, 19104\\ 
  $^2$Dept. of Radiology and Medical Imaging, \\ University of Virginia,
  Charlottesville, VA 22903\\
 $^5$\href{http://www.insightsoftwareconsortium.org/}{http://www.insightsoftwareconsortium.org/}}
\maketitle              
\begin{abstract}
The 2012 Segmentation: Algorithms, Theory and Applications (SATA)
challenge suggests that even subtle variation in registration
performance may impact the outcome of multi-atlas segmentation
algorithms. The 2013 SATA challenge organizers therefore requested
standardized registration that enables entrants to use the same
mappings as input to competing algorithms.  We therefore collaborated
to provide, within a relatively brief window of time, over 22,000
registration results based on Advanced Normalization Tools
(\href{http://stnava.github.io/ANTs/}{ANTs link}).
The diencephalon component of the challenge presented familiar and
easily addressed data requiring only 1,600 mappings between different
3D human T1 neuroimages.  The 3D multiple modality MRI ``dog leg'' dataset ($>$ 7,000
mappings) presented the opportunity to improve performance by using
a multivariate similarity metric.  The 4D cardiac (or CAP) dataset
($\approx$ 13,000 mappings) includes highly variable image quality, anatomy
and field of view.  We detail the ANTs variants that address the most basic brain dataset, where we used
a template-based approach, to the more challenging CAP dataset which
employed a more customized registration solution based on prior
knowledge.  The scripts, source code and a small set of example data
accompany this paper and are available online.  
\footnote{This work is supported by National Library of Medicine sponsored ARRA stimulus
funding.}
\end{abstract}

\begin{comment}
Brian,
 
One of the results of our multi-atlas workshop / challenge was that registration often had a larger impact on method performance than the statistical combination method (no real surprise there: garbage in = garbage out). SyN was used by the top two groups using Arno Klein’s published parameters.
 
In our journal write-up of the workshop results, we are planning on re-running the fusion algorithms using a common registration. Would you be willing to provide the “definitive” SyN results? We have 15 source images (plus label maps) that we need to match pair-wise to 20 target images (without label maps). If you would prefer to specify the command line parameters, Andrew (cc’d) can run the data through on our system.
 
Thanks!
Bennett

Dr. Avants,

After last year's MICCAI whole-brain segmentation challenge, two things became abundantly clear: (1) in order to isolate the benefits of specific label fusion algorithms we need to have a standardized registration and (2) the ANTs registration package represents the premier image registration framework. 

As a result, for this year's challenge (as part of the SATA workshop ) we would like to provide standardized atlas-target registrations for each of the datasets that we are considering.

Would you be willing to contribute these standardized registrations for the challenge?

The workshop website can be found here:
https://masi.vuse.vanderbilt.edu/workshop2013/index.php/Main_Page

The information on each of the datasets can be found here:
https://masi.vuse.vanderbilt.edu/workshop2013/index.php/Segmentation_Challenge_Details

The complete data package can be downloaded here:
https://masi.vuse.vanderbilt.edu/workshop2013/index.php/SATA_Data_Registration_Form

 For this year's challenge we are considering 3 datasets:
(1) A diencephalon (mid-brain) dataset
(2) A canine leg dataset
(3) A cardiac atlas dataset

 ``From my perspective, I think that the mid-brain and canine leg
  datasets would not be terribly difficult to achieve consistent
  correspondence across the images. However, the cardiac atlas dataset
  might be significantly more challenging due to the way in which the
  images are acquired (i.e.,wildly varying orientations and
  fields-of-view).'' ---{\em SATA Organizers}

We would greatly appreciate your help for making this challenge as successful as possible. If you have any questions/comments/concerns please do not hesitate to bring them up.

Thanks again,
Andrew Asman

\end{comment}

\section{Introduction}
Several early papers in image registration / segmentation focus on 
clinical applications including microscopy, nuclear medicine, tracking longitudinal
change and angiography
\cite{Badran1991,Venot1986,Venot1984,Wrigley1982,Adair1981}.  Rapid progress in registration
theory and implementation followed these early developments with
focus, in general, on either similarity metrics \cite{Wells1996},
transformation models \cite{Miller2005} or the combination of these in a general
purpose method \cite{Rueckert1999}.  More recently, as the core technology of segmentation and
registration mature, the research focus has returned to specific clinical
applications such as radiation therapy \cite{Chan2013}, drug discovery \cite{Fox2009},
neurosurgery \cite{Omara2013} and image-based biomarkers of pain \cite{Loggia2013}.  
Quantitative image mapping is also relied upon heavily for automated analysis of large datasets
such as ADNI \cite{Weiner2012} that cannot be processed ``by hand.''

These new problem domains are revealing the limitations of traditional
methods.  Brain registration methods, for instance, often assume that
a single reference space / template is adequate to combine information
across a population and that there exists, roughly, a diffeomorphism
between any pair of individual brains.  Segmentation methods, on the
other hand, often assume that intensity and object smoothness or
texture encode the shape to be segmented or that a single model object
is sufficient to act as a prior.  These assumptions may not
hold when imaging other organs or in emerging MR modalities.  

Multi-atlas segmentation and registration (MASR) methods help overcome
the limitations associated with mapping to a single template or using
a single exemplar segmentation.  These methods encode the notion that
different topology or image contrast may exist between different
subjects and, further, that the user cannot know these subjects {\em a
priori}.  In this case, the problem is to develop algorithms that
automatically adjust for registration accuracy at both global and
local scales and determine segmentation labels that incorporate
confidence.  MASR therefore enables applications that do not depend upon
successful pair-wise registration between all subjects, a goal that is
currently beyond the reach of any general purpose registration tool.

The organizers of the SATA 2013 challenge collected datasets that
frame, with specific examples, the issues raised above.  The
organizers wrote on May 10, 2013 ``in order to isolate the benefits of
specific label fusion algorithms we need to have a standardized
registration.''  The ANTs development team agreed to collaborate on a set of standard registration results for the challenge.  We
felt that our participation was important due to the variety of the challenge
data, the fact that not all entrants are biomedical image registration
experts and the results of SATA 2012 which suggested that a common
transformation basis set would help isolate the differences between
the segmentation component in MASR.  In other words, using a common
set of ``voters'' allows a more direct comparison of the
segmentation/fusion methods that are currently at the heart of the
SATA challenge. We present,
below, the ANTs techniques that address the unique hurdles as well as
opportunities afforded by this diverse collection of images.

\section{Methods} 
Three MRI datasets constitute the driving biological problems for the
2013 challenge: T1-weighted whole head images of the diencephalon
(mid-brain), multi-modality T1 and T2 canine leg images and a 4D
cardiac atlas dataset.  The SATA organizers requested that we provide
transformed label sets and intensity images for all
training$\leftrightarrow$testing and training$\leftrightarrow$training
pairs.  The registration goal was to map the images with ``reasonable
and consistent'' peformance and an open-source system.  

We processed all data with Advanced Normalization Tools ( github
commit e3ba1c527fa63cc524bae7912b415d77adadc165 ) based on ITKv4.  The
processing was organized with a combination of bash or perl scripts
and run through standard Sun Grid Engine distributed computing.
The core methods are available in ANTs programs
\texttt{antsRegistration}, \texttt{ImageMath} (utilities),
\texttt{N4BiasFieldCorrection} (bias correction) and \texttt{Atropos}
(segmentation).  These core tools are described in archived
publications \cite{}.  However, we employed several new additions to
ANTs that are as recent as 2013 and which are based upon our work for
version 4 of the Insight ToolKit.  Furthermore, we will illustrate the
SATA implementations with reproducible examples.

\subsection{Diencephalon/Brain Data}
 \begin{quote}
 ``... the mid-brain and canine leg
  datasets should not be terribly difficult ...'' ---{\em SATA Organizers}
\end{quote}
 
\subsection{Dog Leg Multi-Modality Data}

\subsection{CAP Cardiac Data}
The CAP dataset is more challenging than the other two and includes a volumetric time series of the beating heart. 

 \begin{quote}
  `` ... the cardiac atlas dataset
  might be significantly more challenging due to the way in which the
  images are acquired (i.e.,wildly varying orientations and
  fields-of-view) ... I wouldnt be surprised if getting `reasonable' \& `consistent' registrations for this data is difficult or impossible.'' ---{\em SATA Organizers}
\end{quote}
 
\noindent The organizers also requested that the output labels be
consistent with the original 4D manual labels.  

\begin{figure}[t]
 \centering 
  \includegraphics[width=5in]{../figs/SATA_diencephalon.png}
 \caption{Diencephalon methods}
 \label{fig:DIEmethods}
\end{figure}

\begin{figure}[t]
 \centering 
  \includegraphics[width=5in]{../figs/CAP_methods.pdf}
 \caption{CAP methods}
 \label{fig:CAPmethods}
\end{figure}

\begin{figure}[t]
 \centering 
  \includegraphics[width=5in]{../figs/spider_CAP.pdf}
 \caption{We visualize the variability in pairwise registration
   performance for CAP data.  We quantified variability in
   all 12,865 data points by measuring
   the global correlation of the deformed image and the target image,
   i.e. both training (labeled R$\star$) and testing (labeled E$\star$). ``Spiky''
   distributions of the correlation function indicate that
 a few pairings performed much better/worse than the majority, at
 least at a global level.  This
 was verified by testing the gaussianity of the resulting correlation
 distribution.  As an example, the sixth subject from top left (E6) has a
 highly non-gaussian distribution of similarity metric performance and
results in a significant p-value for the non-gaussianity test ($p <$
1.e-7 ).  Overall, 13.5\% of subjects exhibited non-gaussian
distributions for this function, after multiple comparisons correction by false discovery rate.}
 \label{fig:CAPvar}
\end{figure}

\section{Results}

\subsection{Diencephalon/Brain Data}

\subsection{Dog Leg Multi-Modality Data}

\subsection{CAP Cardiac Data}
We estimated a failure rate of X\% on this data  .... look at
histogram of correlation 

\section{Discussion} The processing decisions that we made in the
standard registration suggest several possible outcomes.  For
instance, our pseudo-geodesic approach focusing on the whole brain
should be suboptimal for the the diencephalon data. Entrants that
choose to perform pairwise registration between all subjects should
find better performance than the standard approach.

The cardiac dataset reveals some of the limitations involved with naive
pair-wise registration and highlights the benefit of
problem-specific strategies and multi-atlas methods.  

The CAP data comes with some caveats.  It is unclear how well the
intra-subject alignment performs.  At the same time, the intra-rater
variability in labeling the time series in unknown; at first glance,
it would be difficult to consistently label the myocardium in 3D over
several dozen or more time frames. 

\bibliographystyle{splncs}
\bibliography{knobsock}
\end{document}



